{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepmind/perception_test/blob/main/baselines/temporal_action_localisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PJx-OzeSK5E"
      },
      "source": [
        "Copyright 2023 DeepMind Technologies Limited\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0).\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tJWYL2oSLgu"
      },
      "source": [
        "# Temporal Action Localisation ActionFormer Baseline\n",
        "\n",
        "Github: https://github.com/deepmind/perception_test\n",
        "\n",
        "## The Perception Test\n",
        "[Perception Test: A Diagnostic Benchmark for Multimodal Video Models](https://arxiv.org/abs/2305.13786) is a multimodal benchmark designed to comprehensively evaluate the perception and reasoning skills of multimodal video models. The Perception Test dataset introduces real-world videos designed to show perceptually interesting situations and defines multiple computational tasks (object and point tracking, action and sound localisation, multiple-choice and grounded video question-answering). Here, we provide details and a baseline for the temporal action localisation task.\n",
        "\n",
        "[![Perception Test Overview Presentation](https://img.youtube.com/vi/8BiajMOBWdk/maxresdefault.jpg)](https://youtu.be/8BiajMOBWdk?t=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pxtHyrHy_4s"
      },
      "source": [
        "\n",
        "## Temporal Action Localisation\n",
        "In the temporal action localisation task, the model receives a video and is required to localise and classify the actions occurring in the video according to a predefined set of classes.\n",
        "\n",
        "The below image shows examples of temporal action localisation annotations. Each action is represented by a set of timestamps representing the start and end of the action along with its corresponding class label.\n",
        "\n",
        "![image](https://storage.googleapis.com/dm-perception-test/img/action_annotations.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlwAXE4nzDA3"
      },
      "source": [
        "## ActionFormer baseline\n",
        "This notebook demonstrates how to use our modified [ActionFormer repository](https://github.com/ptchallenge-workshop/actionformer_release_PT) to train and evaluate an ActionFormer model, for both video only input and combined video and audio input modalities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEZwbpXoweWs"
      },
      "outputs": [],
      "source": [
        "# Make sure GPU runtime is enabled!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_ljvdSjvfOR"
      },
      "outputs": [],
      "source": [
        "# @title Clone adapted ActionFormer repository and install\n",
        "#!git clone https://github.com/ptchallenge-workshop/actionformer_release_PT.git\n",
        "\n",
        "#%cd /content/actionformer_release_PT/libs/utils\n",
        "#!python setup.py install --user\n",
        "#%cd ../..\n",
        "\n",
        "# dirs for storing models and data\n",
        "!cd /home/actionformer_release_PT\n",
        "!mkdir data\n",
        "!mkdir data/pt\n",
        "!mkdir ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0gEbwfQxDiJ"
      },
      "outputs": [],
      "source": [
        "# @title Download Utility Function\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "\n",
        "\n",
        "def download_and_unzip(url: str, destination: str):\n",
        "  \"\"\"Downloads and unzips a .zip file to a destination.\n",
        "\n",
        "  Downloads a file from the specified URL, saves it to the destination\n",
        "  directory, and then extracts its contents.\n",
        "\n",
        "  If the file is larger than 1GB, it will be downloaded in chunks,\n",
        "  and the download progress will be displayed.\n",
        "\n",
        "  Args:\n",
        "    url (str): The URL of the file to download.\n",
        "    destination (str): The destination directory to save the file and\n",
        "      extract its contents.\n",
        "  \"\"\"\n",
        "  if not os.path.exists(destination):\n",
        "    os.makedirs(destination)\n",
        "\n",
        "  filename = url.split('/')[-1]\n",
        "  file_path = os.path.join(destination, filename)\n",
        "\n",
        "  if os.path.exists(file_path):\n",
        "    print(f'{filename} already exists. Skipping download.')\n",
        "    return\n",
        "\n",
        "  response = requests.get(url, stream=True)\n",
        "  total_size = int(response.headers.get('content-length', 0))\n",
        "  gb = 1024*1024*1024\n",
        "\n",
        "  if total_size / gb > 1:\n",
        "    print(f'{filename} is larger than 1GB, downloading in chunks')\n",
        "    chunk_flag = True\n",
        "    chunk_size = int(total_size/100)\n",
        "  else:\n",
        "    chunk_flag = False\n",
        "    chunk_size = total_size\n",
        "\n",
        "  with open(file_path, 'wb') as file:\n",
        "    for chunk_idx, chunk in enumerate(\n",
        "        response.iter_content(chunk_size=chunk_size)):\n",
        "      if chunk:\n",
        "        if chunk_flag:\n",
        "          print(f\"\"\"{chunk_idx}% downloading\n",
        "          {round((chunk_idx*chunk_size)/gb, 1)}GB\n",
        "          / {round(total_size/gb, 1)}GB\"\"\")\n",
        "        file.write(chunk)\n",
        "  print(f\"'{filename}' downloaded successfully.\")\n",
        "\n",
        "  with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination)\n",
        "  print(f\"'{filename}' extracted successfully.\")\n",
        "\n",
        "  os.remove(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoOy1uhUyusl"
      },
      "outputs": [],
      "source": [
        "# @title Download data and pretrained model\n",
        "data_path = './data/pt'\n",
        "model_path = './ckpt'\n",
        "\n",
        "train_annot_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/challenge_action_localisation_train_annotations.zip'\n",
        "download_and_unzip(train_annot_url, data_path)\n",
        "train_video_feat_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/action_localisation_train_video_features.zip'\n",
        "download_and_unzip(train_video_feat_url, data_path)\n",
        "train_audio_feat_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/sound_localisation_train_audio_features.zip'\n",
        "download_and_unzip(train_audio_feat_url, data_path)\n",
        "\n",
        "\n",
        "valid_annot_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/challenge_action_localisation_valid_annotations.zip'\n",
        "download_and_unzip(valid_annot_url, data_path)\n",
        "valid_video_feat_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/action_localisation_valid_video_features.zip'\n",
        "download_and_unzip(valid_video_feat_url, data_path)\n",
        "valid_audio_feat_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/sound_localisation_valid_audio_features.zip'\n",
        "download_and_unzip(valid_audio_feat_url, data_path)\n",
        "\n",
        "# here we download a pretrained model, this can be commented out and the\n",
        "# training command below can be ran instead to train the model from scratch\n",
        "model_url = 'https://storage.googleapis.com/dm-perception-test/saved_models/perception_tal_video_train_reproduce.zip'\n",
        "download_and_unzip(model_url, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uR8pmMJjwx2Y"
      },
      "outputs": [],
      "source": [
        "# @title Training ActionFormer model\n",
        "# Downloading pretrained model in the cell above instead of\n",
        "# training, uncomment the line below to run command for training\n",
        "# !python3 train.py configs/perception_tal_video_train.yaml --output reproduce\n",
        "\n",
        "# multimodal version using audio features alongside video features\n",
        "# !python3 train.py configs/perception_tal_multi_train.yaml --output reproduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OcRUQjRMw5bp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dataset': {'crop_ratio': [0.9, 1.0],\n",
            "             'default_fps': 15,\n",
            "             'downsample_rate': 1,\n",
            "             'feat_folder': './data/pt/action_localisation_valid_video_features',\n",
            "             'feat_stride': 16,\n",
            "             'file_ext': '.npy',\n",
            "             'file_prefix': 'v_',\n",
            "             'force_upsampling': True,\n",
            "             'input_dim': 512,\n",
            "             'input_modality': 'video',\n",
            "             'json_file': './data/pt/challenge_action_localisation_valid.json',\n",
            "             'max_seq_len': 192,\n",
            "             'mm_feat_folder': None,\n",
            "             'num_classes': 63,\n",
            "             'num_frames': 16,\n",
            "             'task': 'action_localisation',\n",
            "             'trunc_thresh': 0.5},\n",
            " 'dataset_name': 'perception',\n",
            " 'devices': ['cuda:0'],\n",
            " 'init_rand_seed': 1234567891,\n",
            " 'loader': {'batch_size': 16, 'num_workers': 4},\n",
            " 'model': {'backbone_arch': (2, 2, 5),\n",
            "           'backbone_type': 'convTransformer',\n",
            "           'embd_dim': 512,\n",
            "           'embd_kernel_size': 3,\n",
            "           'embd_with_ln': True,\n",
            "           'fpn_dim': 512,\n",
            "           'fpn_start_level': 0,\n",
            "           'fpn_type': 'identity',\n",
            "           'fpn_with_ln': True,\n",
            "           'head_dim': 512,\n",
            "           'head_kernel_size': 3,\n",
            "           'head_num_layers': 3,\n",
            "           'head_with_ln': True,\n",
            "           'input_dim': 512,\n",
            "           'max_buffer_len_factor': 1.0,\n",
            "           'max_seq_len': 192,\n",
            "           'n_head': 8,\n",
            "           'n_mha_win_size': [7, 7, 7, 7, 7, -1],\n",
            "           'num_classes': 63,\n",
            "           'regression_range': [(0, 4),\n",
            "                                (4, 8),\n",
            "                                (8, 16),\n",
            "                                (16, 32),\n",
            "                                (32, 64),\n",
            "                                (64, 10000)],\n",
            "           'scale_factor': 2,\n",
            "           'test_cfg': {'duration_thresh': 0.05,\n",
            "                        'ext_score_file': None,\n",
            "                        'iou_threshold': 0.1,\n",
            "                        'max_seg_num': 500,\n",
            "                        'min_score': 0.001,\n",
            "                        'multiclass_nms': True,\n",
            "                        'nms_method': 'soft',\n",
            "                        'nms_sigma': 0.4,\n",
            "                        'pre_nms_thresh': 0.001,\n",
            "                        'pre_nms_topk': 5000,\n",
            "                        'voting_thresh': 0.75},\n",
            "           'train_cfg': {'center_sample': 'radius',\n",
            "                         'center_sample_radius': 1.5,\n",
            "                         'clip_grad_l2norm': 1.0,\n",
            "                         'cls_prior_prob': 0.01,\n",
            "                         'dropout': 0.0,\n",
            "                         'droppath': 0.1,\n",
            "                         'head_empty_cls': [],\n",
            "                         'init_loss_norm': 250,\n",
            "                         'label_smoothing': 0.0,\n",
            "                         'loss_weight': 1.0},\n",
            "           'use_abs_pe': True,\n",
            "           'use_rel_pe': False},\n",
            " 'model_name': 'LocPointTransformer',\n",
            " 'opt': {'epochs': 30,\n",
            "         'learning_rate': 0.001,\n",
            "         'momentum': 0.9,\n",
            "         'schedule_gamma': 0.1,\n",
            "         'schedule_steps': [],\n",
            "         'schedule_type': 'cosine',\n",
            "         'type': 'AdamW',\n",
            "         'warmup': True,\n",
            "         'warmup_epochs': 5,\n",
            "         'weight_decay': 0.05},\n",
            " 'output_folder': './ckpt/',\n",
            " 'test_cfg': {'duration_thresh': 0.05,\n",
            "              'ext_score_file': None,\n",
            "              'iou_threshold': 0.1,\n",
            "              'max_seg_num': 500,\n",
            "              'min_score': 0.001,\n",
            "              'multiclass_nms': True,\n",
            "              'nms_method': 'soft',\n",
            "              'nms_sigma': 0.4,\n",
            "              'pre_nms_thresh': 0.001,\n",
            "              'pre_nms_topk': 5000,\n",
            "              'voting_thresh': 0.75},\n",
            " 'train_cfg': {'center_sample': 'radius',\n",
            "               'center_sample_radius': 1.5,\n",
            "               'clip_grad_l2norm': 1.0,\n",
            "               'cls_prior_prob': 0.01,\n",
            "               'dropout': 0.0,\n",
            "               'droppath': 0.1,\n",
            "               'head_empty_cls': [],\n",
            "               'init_loss_norm': 250,\n",
            "               'label_smoothing': 0.0,\n",
            "               'loss_weight': 1.0},\n",
            " 'train_split': ['train'],\n",
            " 'val_split': ['valid']}\n",
            "=> loading checkpoint 'ckpt/perception_tal_video_train_reproduce/epoch_035.pth.tar'\n",
            "Loading from EMA model ...\n",
            "\n",
            "Start testing model LocPointTransformer ...\n",
            "Test: [00010/05359]\tTime 0.08 (0.08)\n",
            "Test: [00020/05359]\tTime 0.04 (0.06)\n",
            "Test: [00030/05359]\tTime 0.04 (0.06)\n",
            "Test: [00040/05359]\tTime 0.04 (0.05)\n",
            "Test: [00050/05359]\tTime 0.04 (0.05)\n",
            "Test: [00060/05359]\tTime 0.04 (0.05)\n",
            "Test: [00070/05359]\tTime 0.04 (0.05)\n",
            "Test: [00080/05359]\tTime 0.04 (0.05)\n",
            "Test: [00090/05359]\tTime 0.04 (0.05)\n",
            "Test: [00100/05359]\tTime 0.04 (0.05)\n",
            "Test: [00110/05359]\tTime 0.04 (0.05)\n",
            "Test: [00120/05359]\tTime 0.04 (0.05)\n",
            "Test: [00130/05359]\tTime 0.04 (0.05)\n",
            "Test: [00140/05359]\tTime 0.04 (0.05)\n",
            "Test: [00150/05359]\tTime 0.04 (0.05)\n",
            "Test: [00160/05359]\tTime 0.04 (0.05)\n",
            "Test: [00170/05359]\tTime 0.04 (0.05)\n",
            "Test: [00180/05359]\tTime 0.04 (0.05)\n",
            "Test: [00190/05359]\tTime 0.04 (0.05)\n",
            "^C\n",
            "Traceback (most recent call last):\n",
            "  File \"eval.py\", line 128, in <module>\n",
            "    main(args)\n",
            "  File \"eval.py\", line 94, in main\n",
            "    mAP = valid_one_epoch(\n",
            "  File \"/home/actionformer_release_PT/libs/utils/train_utils.py\", line 393, in valid_one_epoch\n",
            "    output = model(video_list)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/data_parallel.py\", line 166, in forward\n",
            "    return self.module(*inputs[0], **kwargs[0])\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/actionformer_release_PT/libs/modeling/meta_archs.py\", line 338, in forward\n",
            "    feats, masks = self.backbone(batched_inputs, batched_masks)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/actionformer_release_PT/libs/modeling/backbones.py\", line 160, in forward\n",
            "    x, mask = self.branch[idx](x, mask)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/actionformer_release_PT/libs/modeling/blocks.py\", line 724, in forward\n",
            "    out, out_mask = self.attn(self.ln1(x), mask)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/actionformer_release_PT/libs/modeling/blocks.py\", line 585, in forward\n",
            "    v, _ = self.value_conv(x, mask)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/home/actionformer_release_PT/libs/modeling/blocks.py\", line 58, in forward\n",
            "    out_conv = out_conv * out_mask.detach()\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# @title Evaluating ActionFormer model\n",
        "\n",
        "# this saves a results JSON file in this location:\n",
        "# /content/actionformer_release_PT/ckpt/perception_video_train_reproduce/eval_results.json\n",
        "# this is in the correct format for submission on the eval.ai challenge\n",
        "# https://eval.ai/web/challenges/challenge-page/2101/overview\n",
        "!python eval.py configs/perception_tal_video_valid.yaml ckpt/perception_tal_video_train_reproduce/\n",
        "\n",
        "# multimodal version using audio features alongside video features\n",
        "# !python eval.py configs/perception_tal_multi_valid.yaml ckpt/perception_tal_multi_train_reproduce/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
